{
"OWASP_LLM_Top_10_2025": {
      "title": "OWASP LLM Top 10 2025",
      "items": [
        {
          "code": "LLM01:2025",
          "title": "Prompt Injection",
          "description": "A Prompt Injection vulnerability occurs when user-provided prompts manipulate an LLM's behavior or bypass its safeguards-either through direct commands or hidden instructions-leading to unauthorized actions or data exposure."
        },
        {
          "code": "LLM02:2025",
          "title": "Sensitive Information Disclosure",
          "description": "The model may unintentionally reveal confidential data-like PII, API keys, internal prompts, or proprietary documents-by regurgitating memorized or ingested information."
        },
        {
          "code": "LLM03:2025",
          "title": "Supply Chain Vulnerabilities",
          "description": "Dependencies on third-party datasets, plugins, libraries, or pre-trained models can introduce backdoors or compromised components into the AI pipeline early on."
        },
        {
          "code": "LLM04:2025",
          "title": "Data and Model Poisoning",
          "description": "Attackers may inject malicious or biased examples into training, fine-tuning, or inference pipelines, embedding backdoors or skewed behaviors into the model."
        },
        {
          "code": "LLM05:2025",
          "title": "Improper Output Handling",
          "description": "Unsanitized or unvalidated LLM outputs-like HTML, SQL, or code-can expose vulnerabilities such as XSS, SQL injection, or remote code execution when integrated into systems."
        },
        {
          "code": "LLM06:2025",
          "title": "Excessive Agency",
          "description": "When LLMs are granted too much autonomy, functionality, or permissions (e.g., calling powerful plugins or tools), they can perform unintended actions-deleting data, sending emails, etc.-without proper checks."
        },
        {
          "code": "LLM07:2025",
          "title": "System Prompt Leakage",
          "description": "Hidden system instructions or guardrails can be extracted by adversarial queries, compromising internal logic, revealing keys, or enabling bypasses."
        },
        {
          "code": "LLM08:2025",
          "title": "Vector and Embedding Weaknesses",
          "description": "Embedding-based systems (RAG/vector DBs) may leak training data, become susceptible to inversion/collision attacks, or be poisoned via corrupted embeddings."
        },
        {
          "code": "LLM09:2025",
          "title": "Misinformation",
          "description": "LLMs may produce false or misleading “hallucinated” content, especially if trained or prompted on unreliable or poisoned sources."
        },
        {
          "code": "LLM10:2025",
          "title": "Unbounded Consumption",
          "description": "Models may overuse resources-API calls, compute, tokens-leading to runaway costs, performance issues, or denial‑of‑service conditions without limits."
        }
      ]
    }
}